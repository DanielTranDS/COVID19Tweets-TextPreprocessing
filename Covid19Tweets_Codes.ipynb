{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing text files and text preprocessing of Covid19 Tweets\n",
    "\n",
    "Text documents, such as crawled web data, are usually comprised of topically coherent text\n",
    "data, which within each topically coherent data, one would expect that the word usage\n",
    "demonstrates more consistent lexical distributions than that across data-set. A linear partition of\n",
    "texts into topic segments can be used for text analysis tasks, such as passage retrieval in IR\n",
    "(information retrieval), document summarization, recommender systems, and learning-to-rank\n",
    "methods.\n",
    "\n",
    "In this project, there are 2 main tasks that I will carry out. \n",
    "\n",
    "In the first task, I will extract data from a very large number of semi-structured text files, each contains thousand of tweets related to Covid19. Then I will transform the extracted data into XML format, following some pre-specified standards. \n",
    "\n",
    "In the second task, it involves text pre-processing, in particular, preprocess a large amount of tweets and convert them into numerical representations (which are suitable\n",
    "for input into recommender-systems/ information-retrieval algorithms)\n",
    "\n",
    "## Table of Content\n",
    "1. [Parsing Text Files](#1)\n",
    "2. [Text Preprocessing](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing Text Files <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "In this section, I attempt to extract data from semi-structured text files in `Covid19Tweets` files. Each text file contains information about the tweets such as \"id\", \"text\", \"created_at\" attributes. My task will be to extract the data and transform the data into XML format with the following elements:\n",
    "- id: 19-digit number\n",
    "- text: the actual tweet\n",
    "- Created_at: date and time that the tweet was created\n",
    "\n",
    "In order to correctly parse data to XML format, we need to understand the structure of XML file, as well as how to parse emoji to XML format, since a lot of tweets contain emoji, which cannot be parsed using normal method like ordinary texts.\n",
    "\n",
    "There are some specification as follows:\n",
    "- The 'id's are unique, so if there are multiple instances of the same tweets, i will only keep 1 of them in the final XML file\n",
    "- Non-English tweets will be filtered out from the dataset and the final XML only contains tweets in English language. \n",
    "\n",
    "Later on, I realize that there are surrogate pairs that need to be handled correctly, so they can be converted into its proper emoji forms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import re\n",
    "import langid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the relative path to the data file that contains all the text tweet files\n",
    "dir_path=\"./Covid19Tweets\"\n",
    "\n",
    "#Create an empty dictionary to store lists of dictionaries of tweets\n",
    "tweet_dict={}\n",
    "for filename in os.listdir(dir_path):\n",
    "    tweet_list=[]\n",
    "    name=\"Covid19Tweets/\"+filename\n",
    "    with open(os.path.join(dir_path,filename),\"r\") as f:\n",
    "        file=open(name,encoding=\"UTF-8\")\n",
    "        for i in file:\n",
    "            file=i\n",
    "        \n",
    "        #Use regex to extract all the smaller dictionaries (now still in string form) into a list\n",
    "        text=re.findall(r\"{(?:(?!\\\"data\\\")).+?}\",file)\n",
    "        \n",
    "        #Filtered out corrupted tweets\n",
    "        error_list=[]\n",
    "        for a_record in text:\n",
    "            if (\"\\\"text\\\"\" not in a_record) and (\"\\\"id\\\"\"not in a_record) and (\"\\\"created_at\\\"\" not in a_record):\n",
    "                error_list.append(a_record)\n",
    "        \n",
    "        #Use list comprehension to retain only uncorrupted tweets\n",
    "        text=[a_record for a_record in text if a_record not in error_list]\n",
    "        \n",
    "        #Retain only tweets that are in English\n",
    "        correct_text=[]\n",
    "        for a_record in text:\n",
    "            if langid.classify(a_record)[0]=='en':\n",
    "                correct_text.append(a_record)\n",
    "        #Use list comprehension to retain only English tweets\n",
    "        text=[a_record for a_record in text if a_record in correct_text]\n",
    "        \n",
    "        #Use eval() function for each element in the list to convert them into proper dictionary. \n",
    "        #There are some entries with unescaped meta characters. Need to take care of these by try and except\n",
    "        for a_record in text:\n",
    "            try:\n",
    "                dictionary=eval(a_record)\n",
    "            except:\n",
    "                a_record=a_record.replace(\"'\",\"‚Äô\")\n",
    "                a_record=a_record.replace(\"\\n\",\"\")\n",
    "                a_record=a_record.replace(\"\\\"\",\"\")\n",
    "            if dictionary[\"id\"] not in [another_rec[\"id\"] for another_rec in tweet_list]:\n",
    "                tweet_list.append(dictionary)\n",
    "                \n",
    "        #Get the proper date which is the first 10 characters of the filename\n",
    "        tweet_date=filename[:10]\n",
    "        \n",
    "        #Now with the empty tweet_dict created earlier, for each sheet(day) of data as a key, the corresponding value is the list \n",
    "        #of dictionaries created above for that day, tweet_list. However, for 1 day, there can be multiple sheets, so if the day \n",
    "        #already existed, we compile the lists of dictionaries of the same days altogether\n",
    "        \n",
    "        if tweet_date not in tweet_dict.keys():\n",
    "            tweet_dict[tweet_date]=tweet_list\n",
    "        else:\n",
    "            tweet_dict[tweet_date]+=tweet_list\n",
    "#Take 10 minutes to finish running this code block, since there are thousands of text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to deal with surrogate pairs. We need to convert these into its \"emoji\" forms and check again if they are classified as English using langid. We only retain those tweets that are classified as English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in tweet_dict.keys():\n",
    "    non_en=[]\n",
    "    for i in range(len(tweet_dict[day])):\n",
    "        tweet_dict[day][i]['text']=tweet_dict[day][i]['text'].encode('utf-16','surrogatepass').decode('utf-16')\n",
    "        if langid.classify(tweet_dict[day][i]['text'])[0]!='en':\n",
    "            non_en.append(tweet_dict[day][i])\n",
    "    tweet_dict[day]=[tweet for tweet in tweet_dict[day] if tweet not in non_en]\n",
    "#Take about 5 minutes to finish running this code block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take another look at this modified `tweet_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'More than a dozen NYC inmates test positive for COVID-19 https://t.co/v9ZqTL2fCu',\n",
       "  'id': '1241583710194950145',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': \"@shytigress @dharmvirjangra9 @GenDADange @GenPanwar @cdrcshekhar @narendravarma49 @JaganNKaushik @URRao10 @nutan_jyot @IndiaKaPrahari @BHARATMACHINE99 @NaniBellary @nalini51purohit @WishMaster2019 @Bharatwashi1 @gouranga1964 @SethiVed @KEYESEN2000 @sinhrann @RulesElsa @J_o_l_i_e @venkatarat @surewrap @Savitritvs @RBhamaria @Kumaran92023000 @Drsunandambal @ravi_sec @kailashkaushik8 @UnchaTiranga @BillionIndian @roydebasis @1PM Boris Johnson tells Britons not to visit parents on Mother's Day because of #coronavirus\\n\\nBoris Johnson\\xa0has urged the British public not to visit their parents on\\xa0Mother‚Äôs Day\\xa0as he warned that the\\xa0NHS\\xa0was in danger of being ‚Äúoverwhelmed‚Äù\\n https://t.co/2P8VsDQFvq\",\n",
       "  'id': '1241583710396272643',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': 'Please Stay at Home n Help US \\n\\nüôèüôèüôè \\n\\n#CoronaUpdatesInIndia \\n#JantaCurfew \\n#CoronavirusPandemic https://t.co/brAKaIaoay',\n",
       "  'id': '1241583710421405697',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': 'https://t.co/ID44pHZAgc',\n",
       "  'id': '1241583711067557890',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': 'Now it time to protect nation against covid-19 \\n#colona #IndiaFightsCorona',\n",
       "  'id': '1241583715093889025',\n",
       "  'created_at': '2020-03-22T04:33:19.000Z'},\n",
       " {'text': '@Luminousthe3rd @evoclock I‚Äôll trust medical experts over random guy on twitter who thinks COVID19 isn‚Äôt a big deal!',\n",
       "  'id': '1241583715513446400',\n",
       "  'created_at': '2020-03-22T04:33:19.000Z'},\n",
       " {'text': 'Have You Been Noticing All The Dozens of Headlines  Featuring The Number 33 of Freemasons In Connection  With The Covid-19 Coronavirus? https://t.co/OmN7PamSbN via @WorldTruthTV',\n",
       "  'id': '1241583718977814530',\n",
       "  'created_at': '2020-03-22T04:33:20.000Z'},\n",
       " {'text': '@Mel_Ankoly @peacerz1 @STPFreak @overrunbydogs Stories of young people getting gravely sick &amp; dying R starting surface.Stories about people &lt;50 coming down w/serious symptoms R making the rounds on social media,along w/questions about whether seemingly healthy young people ought 2B more concerned\\nhttps://t.co/66EzhBzWd0',\n",
       "  'id': '1241583722819923968',\n",
       "  'created_at': '2020-03-22T04:33:21.000Z'},\n",
       " {'text': \"This is what I keep saying and then people look at me like I'm crazy. I wish I was.\\n\\nI take absolutely zero pleasure in saying that America is about to pay a heavy price for beiving in Trump and his cult and administration. https://t.co/cgqkaDRntx\",\n",
       "  'id': '1241583723193208834',\n",
       "  'created_at': '2020-03-22T04:33:21.000Z'},\n",
       " {'text': 'With the kinetics of coronavirus‚Äôs course, I would think we could have meaningful data within a month, that could guide the course of the pandemic that will be unfolding over the next year.',\n",
       "  'id': '1241583723570585601',\n",
       "  'created_at': '2020-03-22T04:33:21.000Z'},\n",
       " {'text': 'When did a bat go to journalism school?\\nhttps://t.co/lli7qvgW0o',\n",
       "  'id': '1241583726909255680',\n",
       "  'created_at': '2020-03-22T04:33:22.000Z'},\n",
       " {'text': 'I can‚Äôt believe my old party @gop put someone up that should have lost to @HillaryClinton-  I‚Äôve never been a fan of hers or @BillClinton- but @realDonaldTrump is neither conservative nor a leader.  We would have been better under her as president.  I can‚Äôt believe I said that. https://t.co/sEUa4EeShZ',\n",
       "  'id': '1241583726938558464',\n",
       "  'created_at': '2020-03-22T04:33:22.000Z'},\n",
       " {'text': 'Loss of taste and smell could indicate coronavirus, say experts https://t.co/uKe0MhrE8N via @MailOnline',\n",
       "  'id': '1241583727425114113',\n",
       "  'created_at': '2020-03-22T04:33:22.000Z'},\n",
       " {'text': 'Exams has been cancelled. Who dafaq wants to spend quarantine doing further maths. https://t.co/DGZHhyq7EQ',\n",
       "  'id': '1241583731065925632',\n",
       "  'created_at': '2020-03-22T04:33:23.000Z'},\n",
       " {'text': \"@ATTHelp That's hilarious, because your garbage company canceled our service appointment and blamed it on COVID19. How does that work?? Now we have no internet and no idea when your company will give a shit and fix it so we can do our jobs again.\",\n",
       "  'id': '1241583731392954368',\n",
       "  'created_at': '2020-03-22T04:33:23.000Z'},\n",
       " {'text': 'Why isn‚Äôt Coronavirus just absolutely destroying India right now?',\n",
       "  'id': '1241583732043190273',\n",
       "  'created_at': '2020-03-22T04:33:23.000Z'},\n",
       " {'text': 'Curfew of the people, by the people, for the people to fight #COVID19. As a responsible citizen, come forward &amp; respond to PM @narendramodi‚Äôs call for Janta Curfew by taking the ‚ÄòI Support Janta Curfew‚Äù pledge. #IndiaFightsCorona https://t.co/BE6Q8YYlyn',\n",
       "  'id': '1241583735243296769',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'},\n",
       " {'text': 'Why Is the Coronavirus Outbreak So Bad in Italy?  https://t.co/EGD2DQbZKc',\n",
       "  'id': '1241583735528542210',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'},\n",
       " {'text': \"How stupid can people be? Most have realised this one ultimate truth - Survival is the most important. All the divisions, laws are to support that and if its not helping, what is the point of this protest? Even Iran has released &gt;20k prisoners for survival's sake!! #JantaCurfew https://t.co/G7OvSnfo2w\",\n",
       "  'id': '1241583735725674501',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'},\n",
       " {'text': 'Spiraling in a cost savings direction.\\n#CoronavirusPandemic \\n#CoronavirusNewYork \\n#ChineseVirus https://t.co/fj2hJlBpG6',\n",
       "  'id': '1241583735738417152',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dict['2020-03-22'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the surrogate pairs are converted into its \"emoji\" form. Now we can start to transform the data into XML format. We need `encode('ascii', 'xmlcharrefreplace')` function and `decode(\"utf-8\")` to pass those tweets above into XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=open(\"Covid19Tweets_parsed.xml\",'w')\n",
    "outfile.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n')\n",
    "outfile.write('<data>\\n')\n",
    "\n",
    "#Start the loop for the tweet_dict\n",
    "for day in tweet_dict.keys():\n",
    "    outfile.write('<tweets date=\\\"'+day+'\\\">')\n",
    "    for i in range(len(tweet_dict[day])):\n",
    "        text=tweet_dict[day][i]['text'].encode('ascii', 'xmlcharrefreplace')\n",
    "        text=text.decode('utf-8')\n",
    "        outfile.write('<tweet id=\\\"'+tweet_dict[day][i]['id']+'\\\">'+text+'</tweet>')\n",
    "    outfile.write('</tweets>')\n",
    "    \n",
    "outfile.write('</data>')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "In this section, with a secondary data file, which contains 80+ sheets of tweets, each sheet with 2000 tweets, I will generate the corpus vocabulary and sort it alphabetically. Afterwards, for each sheet, I calculate the top 100 frequent unigrams and top 100 frequent bigrams. Lastly, I will generate the sparse representation of the excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
