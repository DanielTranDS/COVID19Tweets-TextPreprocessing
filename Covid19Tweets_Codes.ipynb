{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing text files and text preprocessing of Covid19 Tweets\n",
    "\n",
    "Text documents, such as crawled web data, are usually comprised of topically coherent text\n",
    "data, which within each topically coherent data, one would expect that the word usage\n",
    "demonstrates more consistent lexical distributions than that across data-set. A linear partition of\n",
    "texts into topic segments can be used for text analysis tasks, such as passage retrieval in IR\n",
    "(information retrieval), document summarization, recommender systems, and learning-to-rank\n",
    "methods.\n",
    "\n",
    "In this project, there are 2 main tasks that I will carry out. \n",
    "\n",
    "In the first task, I will extract data from a very large number of semi-structured text files, each contains thousand of tweets related to Covid19. Then I will transform the extracted data into XML format, following some pre-specified standards. \n",
    "\n",
    "In the second task, it involves text pre-processing, in particular, preprocess a large amount of tweets and convert them into numerical representations (which are suitable\n",
    "for input into recommender-systems/ information-retrieval algorithms)\n",
    "\n",
    "## Table of Content\n",
    "1. [Parsing Text Files](#1)\n",
    "2. [Text Preprocessing](#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing Text Files <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "In this section, I attempt to extract data from semi-structured text files in `Covid19Tweets` files. Each text file contains information about the tweets such as \"id\", \"text\", \"created_at\" attributes. My task will be to extract the data and transform the data into XML format with the following elements:\n",
    "- id: 19-digit number\n",
    "- text: the actual tweet\n",
    "- Created_at: date and time that the tweet was created\n",
    "\n",
    "In order to correctly parse data to XML format, we need to understand the structure of XML file, as well as how to parse emoji to XML format, since a lot of tweets contain emoji, which cannot be parsed using normal method like ordinary texts.\n",
    "\n",
    "There are some specification as follows:\n",
    "- The 'id's are unique, so if there are multiple instances of the same tweets, i will only keep 1 of them in the final XML file\n",
    "- Non-English tweets will be filtered out from the dataset and the final XML only contains tweets in English language. \n",
    "\n",
    "Later on, I realize that there are surrogate pairs that need to be handled correctly, so they can be converted into its proper emoji forms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import re\n",
    "import langid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the relative path to the data file that contains all the text tweet files\n",
    "dir_path=\"./Covid19Tweets\"\n",
    "\n",
    "#Create an empty dictionary to store lists of dictionaries of tweets\n",
    "tweet_dict={}\n",
    "for filename in os.listdir(dir_path):\n",
    "    tweet_list=[]\n",
    "    name=\"Covid19Tweets/\"+filename\n",
    "    with open(os.path.join(dir_path,filename),\"r\") as f:\n",
    "        file=open(name,encoding=\"UTF-8\")\n",
    "        for i in file:\n",
    "            file=i\n",
    "        \n",
    "        #Use regex to extract all the smaller dictionaries (now still in string form) into a list\n",
    "        text=re.findall(r\"{(?:(?!\\\"data\\\")).+?}\",file)\n",
    "        \n",
    "        #Filtered out corrupted tweets\n",
    "        error_list=[]\n",
    "        for a_record in text:\n",
    "            if (\"\\\"text\\\"\" not in a_record) and (\"\\\"id\\\"\"not in a_record) and (\"\\\"created_at\\\"\" not in a_record):\n",
    "                error_list.append(a_record)\n",
    "        \n",
    "        #Use list comprehension to retain only uncorrupted tweets\n",
    "        text=[a_record for a_record in text if a_record not in error_list]\n",
    "        \n",
    "        #Retain only tweets that are in English\n",
    "        correct_text=[]\n",
    "        for a_record in text:\n",
    "            if langid.classify(a_record)[0]=='en':\n",
    "                correct_text.append(a_record)\n",
    "        #Use list comprehension to retain only English tweets\n",
    "        text=[a_record for a_record in text if a_record in correct_text]\n",
    "        \n",
    "        #Use eval() function for each element in the list to convert them into proper dictionary. \n",
    "        #There are some entries with unescaped meta characters. Need to take care of these by try and except\n",
    "        for a_record in text:\n",
    "            try:\n",
    "                dictionary=eval(a_record)\n",
    "            except:\n",
    "                a_record=a_record.replace(\"'\",\"‚Äô\")\n",
    "                a_record=a_record.replace(\"\\n\",\"\")\n",
    "                a_record=a_record.replace(\"\\\"\",\"\")\n",
    "            if dictionary[\"id\"] not in [another_rec[\"id\"] for another_rec in tweet_list]:\n",
    "                tweet_list.append(dictionary)\n",
    "                \n",
    "        #Get the proper date which is the first 10 characters of the filename\n",
    "        tweet_date=filename[:10]\n",
    "        \n",
    "        #Now with the empty tweet_dict created earlier, for each sheet(day) of data as a key, the corresponding value is the list \n",
    "        #of dictionaries created above for that day, tweet_list. However, for 1 day, there can be multiple sheets, so if the day \n",
    "        #already existed, we compile the lists of dictionaries of the same days altogether\n",
    "        \n",
    "        if tweet_date not in tweet_dict.keys():\n",
    "            tweet_dict[tweet_date]=tweet_list\n",
    "        else:\n",
    "            tweet_dict[tweet_date]+=tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to deal with surrogate pairs. We need to convert these into its \"emoji\" forms and check again if they are classified as English using langid. We only retain those tweets that are classified as English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in tweet_dict.keys():\n",
    "    non_en=[]\n",
    "    for i in range(len(tweet_dict[day])):\n",
    "        tweet_dict[day][i]['text']=tweet_dict[day][i]['text'].encode('utf-16','surrogatepass').decode('utf-16')\n",
    "        if langid.classify(tweet_dict[day][i]['text'])[0]!='en':\n",
    "            non_en.append(tweet_dict[day][i])\n",
    "    tweet_dict[day]=[tweet for tweet in tweet_dict[day] if tweet not in non_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take another look at this modified `tweet_dict` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'More than a dozen NYC inmates test positive for COVID-19 https://t.co/v9ZqTL2fCu',\n",
       "  'id': '1241583710194950145',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': \"@shytigress @dharmvirjangra9 @GenDADange @GenPanwar @cdrcshekhar @narendravarma49 @JaganNKaushik @URRao10 @nutan_jyot @IndiaKaPrahari @BHARATMACHINE99 @NaniBellary @nalini51purohit @WishMaster2019 @Bharatwashi1 @gouranga1964 @SethiVed @KEYESEN2000 @sinhrann @RulesElsa @J_o_l_i_e @venkatarat @surewrap @Savitritvs @RBhamaria @Kumaran92023000 @Drsunandambal @ravi_sec @kailashkaushik8 @UnchaTiranga @BillionIndian @roydebasis @1PM Boris Johnson tells Britons not to visit parents on Mother's Day because of #coronavirus\\n\\nBoris Johnson\\xa0has urged the British public not to visit their parents on\\xa0Mother‚Äôs Day\\xa0as he warned that the\\xa0NHS\\xa0was in danger of being ‚Äúoverwhelmed‚Äù\\n https://t.co/2P8VsDQFvq\",\n",
       "  'id': '1241583710396272643',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': 'Please Stay at Home n Help US \\n\\nüôèüôèüôè \\n\\n#CoronaUpdatesInIndia \\n#JantaCurfew \\n#CoronavirusPandemic https://t.co/brAKaIaoay',\n",
       "  'id': '1241583710421405697',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': 'https://t.co/ID44pHZAgc',\n",
       "  'id': '1241583711067557890',\n",
       "  'created_at': '2020-03-22T04:33:18.000Z'},\n",
       " {'text': 'Now it time to protect nation against covid-19 \\n#colona #IndiaFightsCorona',\n",
       "  'id': '1241583715093889025',\n",
       "  'created_at': '2020-03-22T04:33:19.000Z'},\n",
       " {'text': '@Luminousthe3rd @evoclock I‚Äôll trust medical experts over random guy on twitter who thinks COVID19 isn‚Äôt a big deal!',\n",
       "  'id': '1241583715513446400',\n",
       "  'created_at': '2020-03-22T04:33:19.000Z'},\n",
       " {'text': 'Have You Been Noticing All The Dozens of Headlines  Featuring The Number 33 of Freemasons In Connection  With The Covid-19 Coronavirus? https://t.co/OmN7PamSbN via @WorldTruthTV',\n",
       "  'id': '1241583718977814530',\n",
       "  'created_at': '2020-03-22T04:33:20.000Z'},\n",
       " {'text': '@Mel_Ankoly @peacerz1 @STPFreak @overrunbydogs Stories of young people getting gravely sick &amp; dying R starting surface.Stories about people &lt;50 coming down w/serious symptoms R making the rounds on social media,along w/questions about whether seemingly healthy young people ought 2B more concerned\\nhttps://t.co/66EzhBzWd0',\n",
       "  'id': '1241583722819923968',\n",
       "  'created_at': '2020-03-22T04:33:21.000Z'},\n",
       " {'text': \"This is what I keep saying and then people look at me like I'm crazy. I wish I was.\\n\\nI take absolutely zero pleasure in saying that America is about to pay a heavy price for beiving in Trump and his cult and administration. https://t.co/cgqkaDRntx\",\n",
       "  'id': '1241583723193208834',\n",
       "  'created_at': '2020-03-22T04:33:21.000Z'},\n",
       " {'text': 'With the kinetics of coronavirus‚Äôs course, I would think we could have meaningful data within a month, that could guide the course of the pandemic that will be unfolding over the next year.',\n",
       "  'id': '1241583723570585601',\n",
       "  'created_at': '2020-03-22T04:33:21.000Z'},\n",
       " {'text': 'When did a bat go to journalism school?\\nhttps://t.co/lli7qvgW0o',\n",
       "  'id': '1241583726909255680',\n",
       "  'created_at': '2020-03-22T04:33:22.000Z'},\n",
       " {'text': 'I can‚Äôt believe my old party @gop put someone up that should have lost to @HillaryClinton-  I‚Äôve never been a fan of hers or @BillClinton- but @realDonaldTrump is neither conservative nor a leader.  We would have been better under her as president.  I can‚Äôt believe I said that. https://t.co/sEUa4EeShZ',\n",
       "  'id': '1241583726938558464',\n",
       "  'created_at': '2020-03-22T04:33:22.000Z'},\n",
       " {'text': 'Loss of taste and smell could indicate coronavirus, say experts https://t.co/uKe0MhrE8N via @MailOnline',\n",
       "  'id': '1241583727425114113',\n",
       "  'created_at': '2020-03-22T04:33:22.000Z'},\n",
       " {'text': 'Exams has been cancelled. Who dafaq wants to spend quarantine doing further maths. https://t.co/DGZHhyq7EQ',\n",
       "  'id': '1241583731065925632',\n",
       "  'created_at': '2020-03-22T04:33:23.000Z'},\n",
       " {'text': \"@ATTHelp That's hilarious, because your garbage company canceled our service appointment and blamed it on COVID19. How does that work?? Now we have no internet and no idea when your company will give a shit and fix it so we can do our jobs again.\",\n",
       "  'id': '1241583731392954368',\n",
       "  'created_at': '2020-03-22T04:33:23.000Z'},\n",
       " {'text': 'Why isn‚Äôt Coronavirus just absolutely destroying India right now?',\n",
       "  'id': '1241583732043190273',\n",
       "  'created_at': '2020-03-22T04:33:23.000Z'},\n",
       " {'text': 'Curfew of the people, by the people, for the people to fight #COVID19. As a responsible citizen, come forward &amp; respond to PM @narendramodi‚Äôs call for Janta Curfew by taking the ‚ÄòI Support Janta Curfew‚Äù pledge. #IndiaFightsCorona https://t.co/BE6Q8YYlyn',\n",
       "  'id': '1241583735243296769',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'},\n",
       " {'text': 'Why Is the Coronavirus Outbreak So Bad in Italy?  https://t.co/EGD2DQbZKc',\n",
       "  'id': '1241583735528542210',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'},\n",
       " {'text': \"How stupid can people be? Most have realised this one ultimate truth - Survival is the most important. All the divisions, laws are to support that and if its not helping, what is the point of this protest? Even Iran has released &gt;20k prisoners for survival's sake!! #JantaCurfew https://t.co/G7OvSnfo2w\",\n",
       "  'id': '1241583735725674501',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'},\n",
       " {'text': 'Spiraling in a cost savings direction.\\n#CoronavirusPandemic \\n#CoronavirusNewYork \\n#ChineseVirus https://t.co/fj2hJlBpG6',\n",
       "  'id': '1241583735738417152',\n",
       "  'created_at': '2020-03-22T04:33:24.000Z'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dict['2020-03-22'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the surrogate pairs are converted into its \"emoji\" form. Now we can start to transform the data into XML format. We need `encode('ascii', 'xmlcharrefreplace')` function and `decode(\"utf-8\")` to pass those tweets above into XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=open(\"Covid19Tweets_parsed.xml\",'w')\n",
    "outfile.write('<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n')\n",
    "outfile.write('<data>\\n')\n",
    "\n",
    "#Start the loop for the tweet_dict\n",
    "for day in tweet_dict.keys():\n",
    "    outfile.write('<tweets date=\\\"'+day+'\\\">')\n",
    "    for i in range(len(tweet_dict[day])):\n",
    "        text=tweet_dict[day][i]['text'].encode('ascii', 'xmlcharrefreplace')\n",
    "        text=text.decode('utf-8')\n",
    "        outfile.write('<tweet id=\\\"'+tweet_dict[day][i]['id']+'\\\">'+text+'</tweet>')\n",
    "    outfile.write('</tweets>')\n",
    "    \n",
    "outfile.write('</data>')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "In this section, with a secondary data file, which contains 80+ sheets of tweets, each sheet with 2000 tweets, I will generate the corpus vocabulary and sort it alphabetically. Afterwards, for each sheet, I calculate the top 100 frequent unigrams and top 100 frequent bigrams. Lastly, I will generate the sparse representation of the excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "#For reading dataframe\n",
    "import pandas as pd  \n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "\n",
    "#for tokenization\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "#for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#for computing distribution from a set of word token\n",
    "from nltk.probability import *\n",
    "\n",
    "#for parallel processing\n",
    "from itertools import chain\n",
    "\n",
    "#for extracting n-grams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#for ensuring bigrams are not split into 2 words\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "#for creating sparse matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.excel._base.ExcelFile at 0x266199c2088>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "excel_data=pd.ExcelFile('Covid19Tweets_Part2.xlsx')\n",
    "excel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 81 sheets in this excel file\n",
    "len(excel_data.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create stopword lists from `stopwords_en` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list=()\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords_list=f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create 2 empty dictionaries and an empty list to store values later\n",
    "mydict,mybidict,mylist={},{},[]\n",
    "\n",
    "#We will loop over all the sheets in the excel_data\n",
    "for sheet in excel_data.sheet_names:\n",
    "    dataset=excel_data.parse(sheet)\n",
    "    \n",
    "    #Shift the dataframe to correct position\n",
    "    dataset=dataset.dropna(1,thresh=1)\n",
    "    df=dataset.dropna()\n",
    "    \n",
    "    #Since there are some dataframes that already have the column header correct, run this \"if\" statement\n",
    "    if df.columns.any() not in ['text','id','created_at']:\n",
    "        #Make the first row the column header and remove the redundant first row afterwards\n",
    "        df.columns=df.iloc[0]\n",
    "        df=df[1:]\n",
    "        \n",
    "    #Create a new column \"verified\" which contains the boolean value for if the tweet is in english\n",
    "    df['verified']=df['text'].apply(lambda x:langid.classify(str(x))[0]=='en')\n",
    "    \n",
    "    #Only retain tweets that are in English\n",
    "    df=df[df.verified==True]\n",
    "    \n",
    "    #Combine all the rows in 'text' column into a string and perform case normalization\n",
    "    textstr=str(df['text'].tolist())\n",
    "    textstr=textstr.lower()\n",
    "    \n",
    "    #Use regex expression to tokenize the newly created string\n",
    "    tokenizer=RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    tokens=tokenizer.tokenize(textstr)\n",
    "    \n",
    "    #Add all the tokens created into mylist which will be used to create bigrams later\n",
    "    for i in tokens:\n",
    "        mylist.append(i)\n",
    "        \n",
    "    #add the key-value of sheet and tokens into mybidict dictionary created early so we use it to find out the top frequent\n",
    "    #bigram later on\n",
    "    mybidict[sheet]=[token for token in tokens]\n",
    "    \n",
    "    #Retains the list of tokens after removing stopwords\n",
    "    ind_filtered_tokens=[token for token in tokens if token not in stopwords_list]\n",
    "    \n",
    "    #Store this sheet-name and list of tokens as key-value pairs in mydict\n",
    "    mydict[sheet]=ind_filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to remove context-dependent stop words, then stemming and remove tokens with very short length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create words_df variable to record the number of documents each word appear in, by ensuring that each word appears only once\n",
    "words_df=list(chain.from_iterable([set(value) for value in mydict.values()]))\n",
    "\n",
    "#Create words_tf variable to record the number of times each word appears\n",
    "words_tf=list(chain.from_iterable(mydict.values()))\n",
    "\n",
    "#This gives the document frequency of each word\n",
    "a=FreqDist(words_df)\n",
    "\n",
    "#Create this empty dictionary to store only words that appear in less than 60 documents but more than 5 documents\n",
    "b={}\n",
    "for k,v in a.items():\n",
    "    if v<=60 and v>=5:\n",
    "        b[k]=v\n",
    "\n",
    "#Get all the words collected above into a new list fil_list\n",
    "fil_list=[i for i in words_tf if i in b.keys()]\n",
    "\n",
    "#Perform stemming on each of these words in the newly created list fil_list\n",
    "stemmer=PorterStemmer()\n",
    "stem_fil_token=[stemmer.stem(i) for i in fil_list]\n",
    "    \n",
    "#Retain only tokens with length longer or equal to 3 into a new list filtered_token\n",
    "filtered_token=[token for token in stem_fil_token if len(token)>3 or len(token)==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use PMI measure to find the first 200 meaningful bigrams, concatenate them with an underscore and add them to the filter_token list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures=nltk.collocations.BigramAssocMeasures()\n",
    "finder=nltk.collocations.BigramCollocationFinder.from_words(mylist)\n",
    "meaningful_bi=finder.nbest(bigram_measures.pmi,200)\n",
    "for i in meaningful_bi:\n",
    "    filtered_token.append(i[0]+'_'+i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sort the list in alphabetical order, and also retain only 1 entry per token to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list=sorted(filtered_token)\n",
    "sorted_list=list(dict.fromkeys(sorted_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly for this part, we create a text file to store the output. Each line contains a token in the list, followed by its index in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Vocab.txt','w') as f:\n",
    "    for i in sorted_list:\n",
    "        f.write(i+':'+str(sorted_list.index(i))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a text file to store the top 100 frequent unigrams\n",
    "with open ('top100_unigram.txt','w') as f:\n",
    "    for day in mydict.keys():\n",
    "        stemmed_value=[stemmer.stem(i) for i in mydict[day]]\n",
    "        f.write(day +':'+ str(FreqDist(stemmed_value).most_common(100))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a text file to store the top 100 frequent bigrams\n",
    "with open('top100_bigram.txt','w') as f:\n",
    "    for day in mydict.keys():\n",
    "        bigrams=ngrams(mybidict[day],n=2)\n",
    "        f.write(day+':'+str(FreqDist(bigrams).most_common(100))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we generate Sparse Representation, with count vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we introduce 200 meaninful bigrams into our vocab, we need to use mwetokenizer to make sure those bigrams are not split into 2 individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer=MWETokenizer(meaningful_bi)\n",
    "colloc_tweet=dict((day,mwetokenizer.tokenize(tweet)) for day,tweet in mybidict.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then start a loop to write the text file that has the sparse representation of the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Countvec.txt','w') as f:\n",
    "    for day in colloc_tweet.keys():\n",
    "        \n",
    "        #For each loop, start the line with the date which can be assessed via mydict.keys()\n",
    "        f.write(day+',')\n",
    "        \n",
    "        #join all the tokens of that day as a big string and make a list of 1 element, the whole string\n",
    "        mystr=' '.join(colloc_tweet[day])\n",
    "        list1=[]\n",
    "        list1.append(mystr)\n",
    "        \n",
    "        #Then transform it into feature vectors\n",
    "        data_features=vectorizer.fit_transform(list1)\n",
    "        listword=[]\n",
    "        \n",
    "        #Get all the bigrams for each day\n",
    "        diff=set(colloc_tweet[day])-set(mybidict[day])\n",
    "        \n",
    "        for word,count in zip(set(colloc_tweet[day]),data_features.toarray()[0]):\n",
    "            if count>0:\n",
    "                \n",
    "                #Only stem unigrams \n",
    "                if word not in list(diff):\n",
    "                    word=stemmer.stem(word)\n",
    "                \n",
    "                #Now we can check if the stemmed unigram or the bigram are in the vocab, so we can check for their index\n",
    "                if word in sorted_list:\n",
    "                    #check if the word is in the sorted_list, then write the index of that word, as given in the vocab, and its count\n",
    "                    pair=str(sorted_list.index(word))+':'+str(count)\n",
    "                    listword.append(pair)\n",
    "        f.write(','.join(listword))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
